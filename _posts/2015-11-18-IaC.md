---
layout:     post
title:      Infrastructure as Code - Terraform, Packer, and Puppet
date:       2015-11-18 19:46
type:       post
draft:      true
---

## Introduction

This article is about how to create code to be able to repeatably reproduce an infrastructure that can be used to automatically assign roles to machines through DNS.

This example will use OpenStack as the deployment platform, but with minimal effort it should be possible to modify the Terraform and Packer templates to work with any of their providers such as AWS or GCE.

## Plan

0. build basic image with puppet agent using Packer
0. build a puppetmaster using Terraform
0. build dns infrastructure using Terraform and puppetmaster roles
0. deploy higher order services using puppetmaster with automatic role assignment based on FQDN

## Naming Scheme

In this example the naming scheme for machines will be `<hostname>.<logical_group>.<location>.<domain>.<tld>`.

This naming scheme paired with a [puppet fact](https://github.com/roobert/puppet-fqdn-data) and hiera config allows us to assign roles to machines in a logical group based on FQDN.

## OpenStack Prerequisites

In order for Packer and Terraform to be able to communicate with OpenStack, some environment variables need to be set. Follow the instructions here: http://docs.openstack.org/cli-reference/content/cli_openrc.html

My infrastructure requires that I assign floating ips to OpenStack instances, so I've added the following to my OpenStack RC file to avoid having to repeatedly set it in templates:

```bash
export OS_POOL_NAME="ext-net"
```

Ensure that whenever running a Packer or Terraform command, the OpenStack environment variables have been sourced.

## Packer - Building A Base Image

In this example we'll be using the [OpenStack Debian Jessie image](http://cdimage.debian.org/cdimage/openstack/8.2.0/) available from the [Debian OpenStack cdimage repo](http://cdimage.debian.org/cdimage/openstack/).

I've created a repository with a basic packer template along with some assets to customize the base image and run some tasks on first boot that configure the image for my environment.

The repository: [https://github.com/roobert/packer-openstack-basic](https://github.com/roobert/packer-openstack-basic/)

Here's a copy of the main template:

```json
{
  "builders": [{
    "type": "openstack",
    "ssh_username": "debian",
    "image_name": "debian-jessie-basic",
    "source_image": "7361359e-25c5-4fa6-b421-65884ccc0f16",
    "floating_ip_pool": "ext-net",
    "flavor": "3"
    }
  ],
  "provisioners": [
    {
      "type": "shell",
      "inline": [
        "sudo install -o debian -g debian -d /var/lib/firstboot"
      ]
    },
    {
      "type": "file",
      "source": "assets/",
      "destination": "/var/lib/firstboot/"
    },
    {
      "type": "shell",
      "inline": [
        "sudo mv -v /var/lib/firstboot/firstboot.service /etc/systemd/system/",
        "sudo systemctl enable firstboot"
      ]
    },
    {
      "type": "shell",
      "script": "scripts/openstack-puppet-client.sh",
      "execute_command": "chmod +x {{ .Path }}; {{ .Vars }} sudo -E -S sh '{{ .Path }}'"
    }
  ]
}
```

Things to note:

* Our openstack instances are not directly accessible so a floating ip is assigned for the duration of the build process
* If using a newer or different image, adjust the `source_image_id`

The main purpose of using Packer here is to create an image which has puppet-agent installed, this image can then be used as a base to bootstrap a puppetmaster, and later, puppet clients.

My particular OpenStack environment has a default domain of 'novalocal' set, which I don't want. I've included a systemd init script that runs an init script on first boot which corrects the hosts file and hostname, then deletes itself.

Run packer build:

```
mbp0 /home/rw/git/packer-openstack-basic master ✓ > packer build openstack-basic.json
openstack output will be in this color.

==> openstack: Discovering enabled extensions...
==> openstack: Loading flavor: 3
    openstack: Verified flavor. ID: 3
==> openstack: Creating temporary keypair for this instance...
==> openstack: Launching server...
    openstack: Server ID: 60e85cb5-d7e7-4ce3-843b-4b721b015e54
==> openstack: Waiting for server to become ready...
==> openstack: Creating floating IP...
    openstack: Pool: ext-net
    openstack: Created floating IP: 10.2.134.149
==> openstack: Associating floating IP with server...
    openstack: IP: 10.2.134.149
    openstack: Added floating IP 10.2.134.149 to instance!
==> openstack: Waiting for SSH to become available...
==> openstack: Connected to SSH!
==> openstack: Provisioning with shell script: /tmp/packer-shell980613793
==> openstack: Uploading assets/ => /var/lib/firstboot/
==> openstack: Provisioning with shell script: /tmp/packer-shell720198995
    openstack: ‘/var/lib/firstboot/firstboot.service’ -> ‘/etc/systemd/system/firstboot.service’
    openstack: Created symlink from /etc/systemd/system/multi-user.target.wants/firstboot.service 
      to /etc/systemd/system/firstboot.service.
==> openstack: Provisioning with shell script: scripts/openstack-basic.sh
...
==> openstack: Stopping server...
    openstack: Waiting for server to stop...
==> openstack: Creating the image: debian-jessie-basic
    openstack: Image: 04aab810-18cb-42a4-a622-cccc7998483a
==> openstack: Waiting for image to become ready...
==> openstack: Deleted temporary floating IP 10.2.134.149
==> openstack: Terminating the source server...
==> openstack: Deleting temporary keypair...
Build 'openstack' finished.

==> Builds finished. The artifacts of successful builds are:
--> openstack: An image was created: 04aab810-18cb-42a4-a622-cccc7998483a
```

It should now be possible to build an OpenStack instance based on the image 'debian-jessie-basic'.

## Puppetmaster Prerequisites

Create three git repositories, one for r10k, one to act as the code repository, and one to contain the puppet environment, these will contain the three configurable sets of data for the puppetmaster.

### Repository: `r10k`

Create and commit an `r10k.yaml` config file:

```yaml
:cachedir: '/opt/puppetlabs/r10k/cache'

git:
  private_key: '/root/.ssh/id_rsa-puppetmaster-environments'
  username: 'git'

:sources:
  :puppet-systems:
    remote: 'git@<git_server>:puppet/environments'
    basedir: '/etc/puppetlabs/code/environments'
```

### Repository: `puppet-environments`

Create a repository and run the following commands to create a basic production `environment` hierarchy:

```bash
mkdir -p {modules,site/profile/manifests,site/role/manifests,hiera/nodes}
touch hiera/common.yaml
touch site/profile/manifests/base.pp
touch environment.conf
touch Puppetfile
touch site.pp

# basic configuration and example profile
tee > environment.conf << EOF
manifest = site.pp
modulepath = modules:site
EOF

tee > site.pp << EOF
hiera_include('classes')
EOF

tee > hiera/common.yaml << EOF
classes:
 - 'profile::base'

ntp::servers:
  - 0.us.pool.ntp.org
  - 1.us.pool.ntp.org
EOF

tee > Puppetfile << EOF
forge 'forge.puppetlabs.com'

mod 'puppetlabs/ntp', '4.1.0'
mod 'puppetlabs/stdlib'
mod 'puppetlabs/vcsrepo'

mod 'fqdn_data',
  :git => 'https://github.com/roobert/puppet-fqdn-data.git'
EOF

tee > site/profile/manifests/base.pp << EOF
class profile::base {
  class { '::ntp':  }
}
EOF

tee > hiera/nodes/$(facter -p networking.fqdn) << EOF
classes:
  - profile::base
EOF
```

Rename `master` branch to `production` to conform to what puppet expects the main branch to be called.

```
...
```

### Repository: `code`

Create and commit a `hiera.yaml` configuration file that looks something like this:

```yaml
:backends:
  - yaml

:yaml:
  :datadir: "/etc/puppetlabs/code/environments/%{::environment}/hieradata"

:merge_behavior: deeper

:hierarchy:
  # ident defaults - useful for development
  - "ident/%{fqdn_data.ident}"

  # location/logical_group/node defaults
  - "location/%{fqdn_data.location}/%{fqdn_data.logical_group}/%{trusted.certname}"

  #  node defaults
  - "nodes/%{trusted.certname}"

  # location/group defaults
  - "location/%{fqdn_data.location}/%{fqdn_data.logical_group}"

  # logical group defaults
  - "logical_group/%{fqdn_data.logical_group}"

  # location defaults
  - "location/%{fqdn_data.location}"

  # common defaults
  - common
```

This hiera config makes it possible to assign a role to a machine based on it's hostname.

For example, you could automatically apply a `mesos-slave` role to a node by creating a node with the FQDN `foo0.mesos-slave0.london0.example.com` and the hiera config file `hieradata/logical_group/mesos-slave0.yaml` with the following contents:

```
classes:
  - role::mesos::slave
```

### Keys

Create an ssh key for each repo and add it to the assets dir:

```bash
ssh-keygen -q -N '' -t rsa -b 4096 -C 'id_rsa-puppetmaster-r10k' \
  -f assets/ssh/id_rsa-puppetmaster-r10k

ssh-keygen -q -N ''-t rsa -b 4096 -C 'id_rsa-puppetmaster-code' \
  -f assets/ssh/id_rsa-puppetmaster-code

ssh-keygen -q -N '' -t rsa -b 4096 -C 'id_rsa-puppetmaster-environments' \
  -f assets/ssh/id_rsa-puppetmaster-environments
```

These keys should be able to checkout to a repository called `r10k` and a repository called `code` respectively.

## Building a Puppet Master using TerraForm

Checkout the terraform-puppetmaster repo from here: [https://github.com/roobert/terraform-puppetmaster](https://github.com/roobert/terraform-puppetmaster).

```json
provider "openstack" {}

resource "openstack_compute_floatingip_v2" "puppet" {}

resource "openstack_compute_instance_v2" "puppet_systems0_example_com" {
  name        = "puppet.systems0.systems0.example.com"
  image_name  = "debian-jessie-basic"
  flavor_id   = 3
  floating_ip = "${openstack_compute_floatingip_v2.puppet.address}"
  key_pair    = "debian"
}
```

Change the domain in the file as appropriate.

I have a ssh key pair called 'debian' in OpenStack, since that's the name of the default account, change this if necessary.

Run `terraform plan` and `terraform apply`:

```
mbp0 /home/rw/git/terraform-puppetmaster master ✓ > terraform plan -out terraform.tfplan
Refreshing Terraform state prior to plan...


The Terraform execution plan has been generated and is shown below.
Resources are shown in alphabetical order for quick scanning. Green resources
will be created (or destroyed and then created if an existing resource
exists), yellow resources are being changed in-place, and red resources
will be destroyed.

Your plan was also saved to the path below. Call the "apply" subcommand
with this plan file and Terraform will exactly execute this execution
plan.

Path: terraform.tfplan

+ openstack_compute_floatingip_v2.puppet
    address:     "" => "<computed>"
    fixed_ip:    "" => "<computed>"
    instance_id: "" => "<computed>"
    pool:        "" => "ext-net"
    region:      "" => "your_region"

+ openstack_compute_instance_v2.puppet_systems0_example_com
    access_ip_v4: "" => "<computed>"
    access_ip_v6: "" => "<computed>"
    flavor_id:    "" => "3"
    flavor_name:  "" => "<computed>"
    floating_ip:  "" => "${openstack_compute_floatingip_v2.puppet.address}"
    image_id:     "" => "<computed>"
    image_name:   "" => "debian-jessie-basic"
    key_pair:     "" => "debian"
    name:         "" => "puppet.systems0.example.com"
    network.#:    "" => "<computed>"
    region:       "" => "your_region"


Plan: 2 to add, 0 to change, 0 to destroy.

mbp0 /home/rw/git/terraform-puppetmaster master ✓ > terraform apply terraform.tfplan
openstack_compute_floatingip_v2.puppet: Creating...
  address:     "" => "<computed>"
  fixed_ip:    "" => "<computed>"
  instance_id: "" => "<computed>"
  pool:        "" => "ext-net"
  region:      "" => "your_region"
openstack_compute_floatingip_v2.puppet: Creation complete
openstack_compute_instance_v2.puppet_systems0_example_com: Creating...
  access_ip_v4: "" => "<computed>"
  access_ip_v6: "" => "<computed>"
  flavor_id:    "" => "3"
  flavor_name:  "" => "<computed>"
  floating_ip:  "" => "10.2.134.155"
  image_id:     "" => "<computed>"
  image_name:   "" => "debian-jessie-basic"
  key_pair:     "" => "robw"
  name:         "" => "puppet.systems0.example.com"
  network.#:    "" => "<computed>"
  region:       "" => "brighton"
openstack_compute_instance_v2.puppet_systems0_example_com: Creation complete

Apply complete! Resources: 2 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the `terraform show` command.

State path: terraform.tfstate

mbp0 /home/rw/git/terraform-puppetmaster master ✓ > 
```

## Building DNS

